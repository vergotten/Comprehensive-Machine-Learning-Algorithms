{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPuJFcmFelSnk9vM7iDCUBO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **1. Компьютерное зрение (Computer Vision)**"],"metadata":{"id":"S1hKVGddEPTO"}},{"cell_type":"markdown","source":["This domain often deals with tasks like image classification, object detection, and semantic segmentation. Some commonly used loss functions include:\n","\n","В этой области часто решаются задачи, такие как классификация изображений, обнаружение объектов и семантическая сегментация. Некоторые часто используемые функции потерь включают:"],"metadata":{"id":"IGB1Yt70ECQP"}},{"cell_type":"markdown","source":["## Cross-Entropy Loss"],"metadata":{"id":"d09qF2NsE9SY"}},{"cell_type":"markdown","source":["**Cross-Entropy Loss**: This is used for multi-class classification problems. It measures the difference between two probability distributions.\n","\n","**Потеря кросс-энтропии (Cross-Entropy Loss)**: Применяется для задач многоклассовой классификации. Измеряет разницу между двуми распределениями вероятностей, что позволяет оценить, насколько хорошо модель предсказывает вероятности каждого класса."],"metadata":{"id":"7Re6kqjcEMxB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwYhgSiSEAm4"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","def cross_entropy_loss(y_pred, y_true):\n","    \"\"\"\n","    Manually implemented Cross-Entropy Loss function.\n","\n","    Parameters:\n","    y_pred (numpy.ndarray): A numpy array of shape (N, C) where N is the number of samples and C is the number of classes.\n","                            It represents the predicted probabilities for each class.\n","    y_true (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n","                            It contains the true class labels for each sample.\n","\n","    Returns:\n","    float: The calculated Cross-Entropy loss.\n","    \"\"\"\n","    N = y_pred.shape[0]\n","    log_likelihood = -np.log(y_pred[range(N), y_true])\n","    loss = np.sum(log_likelihood) / N\n","    return loss\n","\n","def cross_entropy_loss_torch(y_pred, y_true):\n","    \"\"\"\n","    Cross-Entropy Loss function using PyTorch's built-in function.\n","\n","    Parameters:\n","    y_pred (torch.Tensor): A tensor of shape (N, C) where N is the number of samples and C is the number of classes.\n","                           It represents the predicted probabilities for each class.\n","    y_true (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n","                           It contains the true class labels for each sample.\n","\n","    Returns:\n","    torch.Tensor: A tensor containing the Cross-Entropy loss.\n","    \"\"\"\n","    loss = nn.CrossEntropyLoss()\n","    return loss(y_pred, y_true)"]},{"cell_type":"markdown","source":["## Binary Cross-Entropy Loss"],"metadata":{"id":"KYEmCQNpFXKl"}},{"cell_type":"markdown","source":["Binary Cross-Entropy Loss: This is used for binary classification problems. It measures the dissimilarity between the predicted probabilities and the true binary labels."],"metadata":{"id":"mAXZkXLKFiGw"}},{"cell_type":"markdown","source":["Бинарная потеря кросс-энтропии (Binary Cross-Entropy Loss): Применяется для задач бинарной классификации. Измеряет несоответствие между предсказанными вероятностями и истинными бинарными метками, что позволяет оценить, насколько хорошо модель предсказывает вероятность принадлежности к положительному классу."],"metadata":{"id":"LBe-bqaTFlRd"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","def binary_cross_entropy_loss(y_pred, y_true):\n","    \"\"\"\n","    Manually implemented Binary Cross-Entropy Loss function.\n","\n","    Parameters:\n","    y_pred (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n","                            It represents the predicted probabilities for each class.\n","    y_true (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n","                            It contains the true class labels for each sample.\n","\n","    Returns:\n","    float: The calculated Binary Cross-Entropy loss.\n","    \"\"\"\n","    N = y_pred.shape[0]\n","    loss = -1/N * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n","    return loss\n","\n","def binary_cross_entropy_loss_torch(y_pred, y_true):\n","    \"\"\"\n","    Binary Cross-Entropy Loss function using PyTorch's built-in function.\n","\n","    Parameters:\n","    y_pred (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n","                           It represents the predicted probabilities for each class.\n","    y_true (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n","                           It contains the true class labels for each sample.\n","\n","    Returns:\n","    torch.Tensor: A tensor containing the Binary Cross-Entropy loss.\n","    \"\"\"\n","    loss = nn.BCELoss()\n","    return loss(y_pred, y_true)"],"metadata":{"id":"KKhCpB9wFkK6","executionInfo":{"status":"ok","timestamp":1705405466179,"user_tz":-180,"elapsed":254,"user":{"displayName":"Maxim Sorokin","userId":"04915505168425669858"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Mean Squared Error"],"metadata":{"id":"i_9XY8rdF0nx"}},{"cell_type":"markdown","source":["Mean Squared Error (MSE): This is used for regression problems. It measures the average of the squares of the errors — that is, the average squared difference between the estimated values and what is estimated."],"metadata":{"id":"XWXlJia0Fza8"}},{"cell_type":"markdown","source":["Среднеквадратичная ошибка (MSE, Mean Squared Error): Применяется для задач регрессии. Измеряет среднее значение квадратов ошибок — то есть среднеквадратичное отклонение между оценочными значениями и тем, что оценивается. Это позволяет оценить, насколько хорошо модель предсказывает непрерывные значения."],"metadata":{"id":"LkejuNkzF3DO"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","def mean_squared_error(y_pred, y_true):\n","    \"\"\"\n","    Manually implemented Mean Squared Error (MSE) loss function.\n","\n","    Parameters:\n","    y_pred (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n","                            It represents the predicted values.\n","    y_true (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n","                            It contains the true values.\n","\n","    Returns:\n","    float: The calculated MSE loss.\n","    \"\"\"\n","    N = y_pred.shape[0]\n","    mse = np.sum((y_true - y_pred)**2) / N\n","    return mse\n","\n","def mean_squared_error_torch(y_pred, y_true):\n","    \"\"\"\n","    Mean Squared Error (MSE) loss function using PyTorch's built-in function.\n","\n","    Parameters:\n","    y_pred (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n","                           It represents the predicted values.\n","    y_true (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n","                           It contains the true values.\n","\n","    Returns:\n","    torch.Tensor: A tensor containing the MSE loss.\n","    \"\"\"\n","    loss = nn.MSELoss()\n","    return loss(y_pred, y_true)"],"metadata":{"id":"DLp8lS1rF3WZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Intersection over Union (IoU)"],"metadata":{"id":"FuRp6DqkILkB"}},{"cell_type":"markdown","source":["Intersection over Union (IoU): This is used for object detection and segmentation. It measures how well a predicted object aligns with the actual object annotation.\n","\n","Пересечение над объединением (IoU, Intersection over Union): Применяется для обнаружения объектов и сегментации. Измеряет, насколько хорошо предсказанный объект соответствует фактической аннотации объекта. Это позволяет оценить, насколько хорошо модель предсказывает положение и форму объектов."],"metadata":{"id":"PAw9ZiwdIHKy"}},{"cell_type":"code","source":["def intersection_over_union(box1, box2):\n","    \"\"\"\n","    box1: list or tuple of 4 elements - (x1, y1, x2, y2) where (x1, y1) is the top-left coordinate and (x2, y2) is the bottom-right coordinate of the first box.\n","    box2: list or tuple of 4 elements - (x1, y1, x2, y2) where (x1, y1) is the top-left coordinate and (x2, y2) is the bottom-right coordinate of the second box.\n","    \"\"\"\n","    # Calculate the (x, y)-coordinates of the intersection rectangle\n","    xA = max(box1[0], box2[0])\n","    yA = max(box1[1], box2[1])\n","    xB = min(box1[2], box2[2])\n","    yB = min(box1[3], box2[3])\n","\n","    # Compute the area of intersection rectangle\n","    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n","\n","    # Compute the area of both the prediction and ground-truth rectangles\n","    box1Area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n","    box2Area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n","\n","    # Compute the intersection over union by taking the intersection area and dividing it by the sum of prediction + ground-truth areas - the intersection area\n","    iou = interArea / float(box1Area + box2Area - interArea)\n","\n","    # Return the intersection over union value\n","    return iou"],"metadata":{"id":"F8NU-0mTINAV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Hinge Loss"],"metadata":{"id":"SR3MWh4BIszO"}},{"cell_type":"markdown","source":["Hinge Loss: This is used for “maximum-margin” classification, such as in Support Vector Machines (SVMs). It measures the difference between the predicted and the actual output."],"metadata":{"id":"TvEmhXYvIHNH"}},{"cell_type":"markdown","source":["Потеря на петлях (Hinge Loss): Применяется для классификации с “максимальным зазором”, например, в методе опорных векторов (SVM). Измеряет разницу между предсказанным и фактическим выходом, что позволяет оценить, насколько хорошо модель разделяет классы."],"metadata":{"id":"cKdFAwloIHPW"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","def hinge_loss(y_pred, y_true):\n","    \"\"\"\n","    Manually implemented Hinge Loss function.\n","\n","    Parameters:\n","    y_pred (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n","                            It represents the predicted values.\n","    y_true (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n","                            It contains the true values.\n","\n","    Returns:\n","    float: The calculated Hinge loss.\n","    \"\"\"\n","    N = y_pred.shape[0]\n","    loss = np.sum(np.maximum(0, 1 - y_true * y_pred)) / N\n","    return loss\n","\n","def hinge_loss_torch(y_pred, y_true):\n","    \"\"\"\n","    Hinge Loss function using PyTorch's built-in function.\n","\n","    Parameters:\n","    y_pred (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n","                           It represents the predicted values.\n","    y_true (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n","                           It contains the true values.\n","\n","    Returns:\n","    torch.Tensor: A tensor containing the Hinge loss.\n","    \"\"\"\n","    loss = nn.MarginRankingLoss(margin=1.0)\n","    y_true[y_true == 0] = -1  # Change the label 0 to -1\n","    return loss(y_pred, y_true, torch.ones_like(y_true))"],"metadata":{"id":"FFRAexlnIvFw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **2. Обработка естественного языка (NLP, Natural Language Processing)**"],"metadata":{"id":"a_ODyeSZJFbG"}},{"cell_type":"markdown","source":["This domain includes tasks like text classification, language translation, and sentiment analysis. Some commonly used loss functions include:"],"metadata":{"id":"qiOpRLk4JEKg"}},{"cell_type":"markdown","source":["В этой области решаются задачи, такие как классификация текста, перевод языка и анализ тональности. Некоторые часто используемые функции потерь включают:"],"metadata":{"id":"MhQx1r9sKON-"}},{"cell_type":"markdown","source":["## Cross-Entropy Loss"],"metadata":{"id":"u77URsclNtiK"}},{"cell_type":"markdown","source":["Cross-Entropy Loss: This is used for language modeling and machine translation. It measures the difference between two probability distributions."],"metadata":{"id":"o8gOodYbNqZ1"}},{"cell_type":"markdown","source":["Потеря кросс-энтропии (Cross-Entropy Loss): Применяется для моделирования языка и машинного перевода. Измеряет разницу между двуми распределениями вероятностей, что позволяет оценить, насколько хорошо модель предсказывает вероятности следующего слова в последовательности."],"metadata":{"id":"CchQrbaHNwwG"}},{"cell_type":"markdown","source":["## Negative Log-Likelihood (NLL)"],"metadata":{"id":"-38vEn_8OBEQ"}},{"cell_type":"markdown","source":["Negative Log-Likelihood (NLL): This is used when models output the log-probability of classes. It measures the sum of the logarithm of probabilities."],"metadata":{"id":"kllyPlFmN56a"}},{"cell_type":"markdown","source":["Отрицательное логарифмическое правдоподобие (NLL, Negative Log-Likelihood): Применяется, когда модели выводят логарифм вероятности классов. Измеряет сумму логарифма вероятностей, что позволяет оценить, насколько хорошо модель предсказывает вероятности классов."],"metadata":{"id":"3ycd7-rvN_yB"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","def negative_log_likelihood_loss_manual(y_pred, y_true):\n","    \"\"\"\n","    Manually implemented Negative Log-Likelihood (NLL) Loss function.\n","\n","    Parameters:\n","    y_pred (numpy.ndarray): A numpy array of shape (N, C) where N is the number of samples and C is the number of classes.\n","                            It represents the log-probabilities of each class.\n","    y_true (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n","                            It contains the true class labels for each sample.\n","\n","    Returns:\n","    float: The calculated NLL loss.\n","    \"\"\"\n","    N = y_pred.shape[0]\n","    nll_loss = -np.sum(y_pred[range(N), y_true]) / N\n","    return nll_loss\n","\n","def negative_log_likelihood_loss_torch(y_pred, y_true):\n","    \"\"\"\n","    Negative Log-Likelihood (NLL) Loss function using PyTorch's built-in function.\n","\n","    Parameters:\n","    y_pred (torch.Tensor): A tensor of shape (N, C) where N is the number of samples and C is the number of classes.\n","                           It represents the log-probabilities of each class.\n","    y_true (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n","                           It contains the true class labels for each sample.\n","\n","    Returns:\n","    torch.Tensor: A tensor containing the NLL loss.\n","    \"\"\"\n","    loss = nn.NLLLoss()\n","    return loss(y_pred, y_true)"],"metadata":{"id":"0H0itMoTNozw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Hinge Loss"],"metadata":{"id":"Xjo8M1YNOkt8"}},{"cell_type":"markdown","source":["Hinge Loss: This is used for text classification problems. It measures the difference between the predicted and the actual output."],"metadata":{"id":"U84ef2zYOavR"}},{"cell_type":"markdown","source":["Потеря на петлях (Hinge Loss): Применяется для задач классификации текста. Измеряет разницу между предсказанным и фактическим выходом, что позволяет оценить, насколько хорошо модель разделяет классы."],"metadata":{"id":"FCpDBmDgOdJW"}},{"cell_type":"markdown","source":["# 3. **Обучение с подкреплением (Reinforcement Learning)**"],"metadata":{"id":"ejNx2-CxP4p9"}},{"cell_type":"markdown","source":["This domain includes tasks like text classification, language translation, and sentiment analysis. Some commonly used loss functions include:"],"metadata":{"id":"mGr7O4VwO6P9"}},{"cell_type":"markdown","source":["\n","В этой области происходит обучение агента принимать последовательность решений. Некоторые часто используемые функции потерь включают:"],"metadata":{"id":"lcZRhotEO2av"}},{"cell_type":"markdown","source":["## Mean Squared Error (MSE)"],"metadata":{"id":"N4QrNzhaPMFp"}},{"cell_type":"markdown","source":["Mean Squared Error (MSE): This is used for estimating the value function. It measures the average of the squares of the errors — that is, the average squared difference between the estimated values and what is estimated."],"metadata":{"id":"FZXPI-gvPCbO"}},{"cell_type":"markdown","source":["Среднеквадратичная ошибка (MSE, Mean Squared Error): Применяется для оценки функции стоимости. Измеряет среднее значение квадратов ошибок — то есть среднеквадратичное отклонение между оценочными значениями и тем, что оценивается. Это позволяет оценить, насколько хорошо модель предсказывает ожидаемую награду."],"metadata":{"id":"_cejOv2xPEjg"}},{"cell_type":"markdown","source":["## Cross-Entropy Loss"],"metadata":{"id":"AurXWNwWPVif"}},{"cell_type":"markdown","source":["**Cross-Entropy Loss**: This is used for multi-class classification problems. It measures the difference between two probability distributions.\n","\n","**Потеря кросс-энтропии (Cross-Entropy Loss)**: Применяется для задач многоклассовой классификации. Измеряет разницу между двуми распределениями вероятностей, что позволяет оценить, насколько хорошо модель предсказывает вероятности каждого класса."],"metadata":{"id":"IEUjXBSbPYZF"}},{"cell_type":"markdown","source":["# **4. Генеративные модели (Generative Models)**"],"metadata":{"id":"lPQDJ4EGP02l"}},{"cell_type":"markdown","source":["\n","В этой области происходит генерация новых экземпляров данных. Некоторые часто используемые функции потерь включают:\n","\n","This domain involves generating new data instances. Some commonly used loss functions include:"],"metadata":{"id":"fLnwxCbWPu50"}},{"cell_type":"markdown","source":["## Kullback-Leibler (KL) Divergence"],"metadata":{"id":"pKKcsVPkQIdz"}},{"cell_type":"markdown","source":["Kullback-Leibler (KL) Divergence: This is used in Variational Autoencoders (VAEs). It measures how one probability distribution differs from another.\n","\n","Дивергенция Кульбака-Лейблера (KL, Kullback-Leibler Divergence): Применяется в вариационных автоэнкодерах (VAE). Измеряет, насколько одно распределение вероятностей отличается от другого, что позволяет оценить, насколько хорошо модель предсказывает распределение данных."],"metadata":{"id":"RbAsLeUEQDL-"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","def kl_divergence_manual(p, q):\n","    \"\"\"\n","    Manually implemented Kullback-Leibler (KL) Divergence function.\n","\n","    Parameters:\n","    p (numpy.ndarray): A numpy array representing the true probability distribution.\n","    q (numpy.ndarray): A numpy array representing the approximated probability distribution.\n","\n","    Returns:\n","    float: The calculated KL divergence.\n","    \"\"\"\n","    return np.sum(p * np.log(p / q))\n","\n","def kl_divergence_torch(p, q):\n","    \"\"\"\n","    Kullback-Leibler (KL) Divergence function using PyTorch's built-in function.\n","\n","    Parameters:\n","    p (torch.Tensor): A tensor representing the true probability distribution.\n","    q (torch.Tensor): A tensor representing the approximated probability distribution.\n","\n","    Returns:\n","    torch.Tensor: A tensor containing the KL divergence.\n","    \"\"\"\n","    kl_div = torch.nn.KLDivLoss(reduction='batchmean')\n","    return kl_div(torch.log(q), p)"],"metadata":{"id":"wFL1VYwWQCxM","executionInfo":{"status":"ok","timestamp":1705408227615,"user_tz":-180,"elapsed":266,"user":{"displayName":"Maxim Sorokin","userId":"04915505168425669858"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Wasserstein Loss"],"metadata":{"id":"wkNPieVTQYFV"}},{"cell_type":"markdown","source":["Wasserstein Loss: This is used in Wasserstein GANs. It measures the distance between the data distribution observed in the training dataset and the distribution observed in the generated examples.\n","\n","Потеря Вассерштейна (Wasserstein Loss): Применяется в Вассерштейновских GAN. Измеряет расстояние между распределением данных, наблюдаемым в обучающем наборе данных, и распределением, наблюдаемым в сгенерированных примерах, что позволяет оценить, насколько хорошо модель воспроизводит распределение данных."],"metadata":{"id":"khFVctSMQXKs"}},{"cell_type":"code","source":["import torch\n","\n","def wasserstein_loss(y_pred, y_true):\n","    \"\"\"\n","    Wasserstein Loss function used in Wasserstein GANs.\n","\n","    Parameters:\n","    y_pred (torch.Tensor): A tensor representing the predicted values.\n","    y_true (torch.Tensor): A tensor containing the true values.\n","\n","    Returns:\n","    torch.Tensor: A tensor containing the Wasserstein loss.\n","    \"\"\"\n","    return torch.mean(y_true * y_pred)"],"metadata":{"id":"qEo3mlGnQWiH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Binary Cross-Entropy Loss"],"metadata":{"id":"sx4xUkcrQ0ss"}},{"cell_type":"markdown","source":["Binary Cross-Entropy Loss: This is used in Generative Adversarial Networks (GANs). It measures the dissimilarity between the predicted probabilities and the true binary labels.\n","\n","Бинарная потеря кросс-энтропии (Binary Cross-Entropy Loss): Применяется в генеративно-состязательных сетях (GAN). Измеряет несоответствие между предсказанными вероятностями и истинными бинарными метками, что позволяет оценить, насколько хорошо модель предсказывает истинность сгенерированных данных."],"metadata":{"id":"7orPDMccQwjF"}},{"cell_type":"markdown","source":["# **5. Области решения задач (Problem-Solving Domains)**"],"metadata":{"id":"pO59hyqMQ7i0"}},{"cell_type":"markdown","source":["These domains focus on using machine learning to solve specific problems. The choice of loss function depends on the specific task. For example:\n","\n","Эти области сосредоточены на использовании машинного обучения для решения конкретных задач. Выбор функции потерь зависит от конкретной задачи. Например:\n","\n","\n"],"metadata":{"id":"0gQAo2chQ6rr"}},{"cell_type":"markdown","source":["## Mean Squared Error (MSE)"],"metadata":{"id":"dbzvBahVRTok"}},{"cell_type":"markdown","source":["Mean Squared Error (MSE): This is used for regression tasks. It measures the average of the squares of the errors — that is, the average squared difference between the estimated values and what is estimated.\n","\n","\n","Среднеквадратичная ошибка (MSE, Mean Squared Error): Применяется для задач регрессии. Измеряет среднее значение квадратов ошибок — то есть среднеквадратичное отклонение между оценочными значениями и тем, что оценивается. Это позволяет оценить, насколько хорошо модель предсказывает непрерывные значения."],"metadata":{"id":"1koi1LnfRSlA"}},{"cell_type":"markdown","source":["## Cross-Entropy Loss"],"metadata":{"id":"bHThB267RVL-"}},{"cell_type":"markdown","source":["Cross-Entropy Loss: This is used for classification tasks. It measures the difference between two probability distributions.\n","\n","\n","Потеря кросс-энтропии (Cross-Entropy Loss): Применяется для задач классификации. Измеряет разницу между двуми распределениями вероятностей, что позволяет оценить, насколько хорошо модель предсказывает вероятности каждого класса.\n"],"metadata":{"id":"8-Skysx3RIwu"}},{"cell_type":"markdown","source":["## Hinge Loss"],"metadata":{"id":"1jbQnRhERWsq"}},{"cell_type":"markdown","source":["Hinge Loss: This is used for “maximum-margin” classification. It measures the difference between the predicted and the actual output.\n","\n","Потеря на петлях (Hinge Loss): Применяется для классификации с “максимальным зазором”. Измеряет разницу между предсказанным и фактическим выходом, что позволяет оценить, насколько хорошо модель разделяет классы."],"metadata":{"id":"nFb-6CoiRI49"}}]}
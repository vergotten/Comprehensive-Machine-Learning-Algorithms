{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1hKVGddEPTO"
   },
   "source": [
    "# **1. Компьютерное зрение (Computer Vision)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGB1Yt70ECQP"
   },
   "source": [
    "This domain often deals with tasks like image classification, object detection, and semantic segmentation. Some commonly used loss functions include:\n",
    "\n",
    "В этой области часто решаются задачи, такие как классификация изображений, обнаружение объектов и семантическая сегментация. Некоторые часто используемые функции потерь включают:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d09qF2NsE9SY"
   },
   "source": [
    "## Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Re6kqjcEMxB"
   },
   "source": [
    "**Cross-Entropy Loss**: This is used for multi-class classification problems. It measures the difference between two probability distributions.\n",
    "\n",
    "**Потеря кросс-энтропии (Cross-Entropy Loss)**: Применяется для задач многоклассовой классификации. Измеряет разницу между двуми распределениями вероятностей, что позволяет оценить, насколько хорошо модель предсказывает вероятности каждого класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwYhgSiSEAm4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Manually implemented Cross-Entropy Loss function.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred (numpy.ndarray): A numpy array of shape (N, C) where N is the number of samples and C is the number of classes.\n",
    "                            It represents the predicted probabilities for each class.\n",
    "    y_true (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n",
    "                            It contains the true class labels for each sample.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated Cross-Entropy loss.\n",
    "    \"\"\"\n",
    "    N = y_pred.shape[0]\n",
    "    log_likelihood = -np.log(y_pred[range(N), y_true])\n",
    "    loss = np.sum(log_likelihood) / N\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_loss_torch(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Cross-Entropy Loss function using PyTorch's built-in function.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred (torch.Tensor): A tensor of shape (N, C) where N is the number of samples and C is the number of classes.\n",
    "                           It represents the predicted probabilities for each class.\n",
    "    y_true (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n",
    "                           It contains the true class labels for each sample.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A tensor containing the Cross-Entropy loss.\n",
    "    \"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYEmCQNpFXKl"
   },
   "source": [
    "## Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAXZkXLKFiGw"
   },
   "source": [
    "Binary Cross-Entropy Loss: This is used for binary classification problems. It measures the dissimilarity between the predicted probabilities and the true binary labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBe-bqaTFlRd"
   },
   "source": [
    "Бинарная потеря кросс-энтропии (Binary Cross-Entropy Loss): Применяется для задач бинарной классификации. Измеряет несоответствие между предсказанными вероятностями и истинными бинарными метками, что позволяет оценить, насколько хорошо модель предсказывает вероятность принадлежности к положительному классу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1705405466179,
     "user": {
      "displayName": "Maxim Sorokin",
      "userId": "04915505168425669858"
     },
     "user_tz": -180
    },
    "id": "KKhCpB9wFkK6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def binary_cross_entropy_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Manually implemented Binary Cross-Entropy Loss function.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n",
    "                            It represents the predicted probabilities for each class.\n",
    "    y_true (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n",
    "                            It contains the true class labels for each sample.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated Binary Cross-Entropy loss.\n",
    "    \"\"\"\n",
    "    N = y_pred.shape[0]\n",
    "    loss = -1/N * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "def binary_cross_entropy_loss_torch(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy Loss function using PyTorch's built-in function.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n",
    "                           It represents the predicted probabilities for each class.\n",
    "    y_true (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n",
    "                           It contains the true class labels for each sample.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A tensor containing the Binary Cross-Entropy loss.\n",
    "    \"\"\"\n",
    "    loss = nn.BCELoss()\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_9XY8rdF0nx"
   },
   "source": [
    "## Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWXlJia0Fza8"
   },
   "source": [
    "Mean Squared Error (MSE): This is used for regression problems. It measures the average of the squares of the errors — that is, the average squared difference between the estimated values and what is estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkejuNkzF3DO"
   },
   "source": [
    "Среднеквадратичная ошибка (MSE, Mean Squared Error): Применяется для задач регрессии. Измеряет среднее значение квадратов ошибок — то есть среднеквадратичное отклонение между оценочными значениями и тем, что оценивается. Это позволяет оценить, насколько хорошо модель предсказывает непрерывные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLp8lS1rF3WZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Manually implemented Mean Squared Error (MSE) loss function.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n",
    "                            It represents the predicted values.\n",
    "    y_true (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n",
    "                            It contains the true values.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated MSE loss.\n",
    "    \"\"\"\n",
    "    N = y_pred.numel() # N = y_pred.shape[0]\n",
    "    mse = torch.sum((y_true - y_pred)**2) / N\n",
    "    return mse\n",
    "\n",
    "def mean_squared_error_torch(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Mean Squared Error (MSE) loss function using PyTorch's built-in function.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n",
    "                           It represents the predicted values.\n",
    "    y_true (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n",
    "                           It contains the true values.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A tensor containing the MSE loss.\n",
    "    \"\"\"\n",
    "    loss = nn.MSELoss()\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuRp6DqkILkB"
   },
   "source": [
    "## Intersection over Union (IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAw9ZiwdIHKy"
   },
   "source": [
    "Intersection over Union (IoU): This is used for object detection and segmentation. It measures how well a predicted object aligns with the actual object annotation.\n",
    "\n",
    "Пересечение над объединением (IoU, Intersection over Union): Применяется для обнаружения объектов и сегментации. Измеряет, насколько хорошо предсказанный объект соответствует фактической аннотации объекта. Это позволяет оценить, насколько хорошо модель предсказывает положение и форму объектов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8NU-0mTINAV"
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(box1, box2):\n",
    "    \"\"\"\n",
    "    box1: list or tuple of 4 elements - (x1, y1, x2, y2) where (x1, y1) is the top-left coordinate and (x2, y2) is the bottom-right coordinate of the first box.\n",
    "    box2: list or tuple of 4 elements - (x1, y1, x2, y2) where (x1, y1) is the top-left coordinate and (x2, y2) is the bottom-right coordinate of the second box.\n",
    "    \"\"\"\n",
    "    # Calculate the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "\n",
    "    # Compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    # Compute the area of both the prediction and ground-truth rectangles\n",
    "    box1Area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    box2Area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "\n",
    "    # Compute the intersection over union by taking the intersection area and dividing it by the sum of prediction + ground-truth areas - the intersection area\n",
    "    iou = interArea / float(box1Area + box2Area - interArea)\n",
    "\n",
    "    # Return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SR3MWh4BIszO"
   },
   "source": [
    "## Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvEmhXYvIHNH"
   },
   "source": [
    "Hinge Loss: This is used for “maximum-margin” classification, such as in Support Vector Machines (SVMs). It measures the difference between the predicted and the actual output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKdFAwloIHPW"
   },
   "source": [
    "Потеря на петлях (Hinge Loss): Применяется для классификации с “максимальным зазором”, например, в методе опорных векторов (SVM). Измеряет разницу между предсказанным и фактическим выходом, что позволяет оценить, насколько хорошо модель разделяет классы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFRAexlnIvFw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def hinge_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Manually implemented Hinge Loss function.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n",
    "                            It represents the predicted values.\n",
    "    y_true (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n",
    "                            It contains the true values.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated Hinge loss.\n",
    "    \"\"\"\n",
    "    N = y_pred.shape[0]\n",
    "    loss = np.sum(np.maximum(0, 1 - y_true * y_pred)) / N\n",
    "    return loss\n",
    "\n",
    "def hinge_loss_torch(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Hinge Loss function using PyTorch's built-in function.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n",
    "                           It represents the predicted values.\n",
    "    y_true (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n",
    "                           It contains the true values.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A tensor containing the Hinge loss.\n",
    "    \"\"\"\n",
    "    loss = nn.MarginRankingLoss(margin=1.0)\n",
    "    y_true[y_true == 0] = -1  # Change the label 0 to -1\n",
    "    return loss(y_pred, y_true, torch.ones_like(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_ODyeSZJFbG"
   },
   "source": [
    "# **2. Обработка естественного языка (NLP, Natural Language Processing)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiOpRLk4JEKg"
   },
   "source": [
    "This domain includes tasks like text classification, language translation, and sentiment analysis. Some commonly used loss functions include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhQx1r9sKON-"
   },
   "source": [
    "В этой области решаются задачи, такие как классификация текста, перевод языка и анализ тональности. Некоторые часто используемые функции потерь включают:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u77URsclNtiK"
   },
   "source": [
    "## Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8gOodYbNqZ1"
   },
   "source": [
    "Cross-Entropy Loss: This is used for language modeling and machine translation. It measures the difference between two probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CchQrbaHNwwG"
   },
   "source": [
    "Потеря кросс-энтропии (Cross-Entropy Loss): Применяется для моделирования языка и машинного перевода. Измеряет разницу между двуми распределениями вероятностей, что позволяет оценить, насколько хорошо модель предсказывает вероятности следующего слова в последовательности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-38vEn_8OBEQ"
   },
   "source": [
    "## Negative Log-Likelihood (NLL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kllyPlFmN56a"
   },
   "source": [
    "Negative Log-Likelihood (NLL): This is used when models output the log-probability of classes. It measures the sum of the logarithm of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ycd7-rvN_yB"
   },
   "source": [
    "Отрицательное логарифмическое правдоподобие (NLL, Negative Log-Likelihood): Применяется, когда модели выводят логарифм вероятности классов. Измеряет сумму логарифма вероятностей, что позволяет оценить, насколько хорошо модель предсказывает вероятности классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0H0itMoTNozw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def negative_log_likelihood_loss_manual(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Manually implemented Negative Log-Likelihood (NLL) Loss function.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred (numpy.ndarray): A numpy array of shape (N, C) where N is the number of samples and C is the number of classes.\n",
    "                            It represents the log-probabilities of each class.\n",
    "    y_true (numpy.ndarray): A numpy array of shape (N,) where N is the number of samples.\n",
    "                            It contains the true class labels for each sample.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated NLL loss.\n",
    "    \"\"\"\n",
    "    N = y_pred.shape[0]\n",
    "    nll_loss = -np.sum(y_pred[range(N), y_true]) / N\n",
    "    return nll_loss\n",
    "\n",
    "def negative_log_likelihood_loss_torch(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Negative Log-Likelihood (NLL) Loss function using PyTorch's built-in function.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred (torch.Tensor): A tensor of shape (N, C) where N is the number of samples and C is the number of classes.\n",
    "                           It represents the log-probabilities of each class.\n",
    "    y_true (torch.Tensor): A tensor of shape (N,) where N is the number of samples.\n",
    "                           It contains the true class labels for each sample.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A tensor containing the NLL loss.\n",
    "    \"\"\"\n",
    "    loss = nn.NLLLoss()\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xjo8M1YNOkt8"
   },
   "source": [
    "## Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U84ef2zYOavR"
   },
   "source": [
    "Hinge Loss: This is used for text classification problems. It measures the difference between the predicted and the actual output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCpDBmDgOdJW"
   },
   "source": [
    "Потеря на петлях (Hinge Loss): Применяется для задач классификации текста. Измеряет разницу между предсказанным и фактическим выходом, что позволяет оценить, насколько хорошо модель разделяет классы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejNx2-CxP4p9"
   },
   "source": [
    "# 3. **Обучение с подкреплением (Reinforcement Learning)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGr7O4VwO6P9"
   },
   "source": [
    "This domain includes tasks like text classification, language translation, and sentiment analysis. Some commonly used loss functions include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcZRhotEO2av"
   },
   "source": [
    "\n",
    "В этой области происходит обучение агента принимать последовательность решений. Некоторые часто используемые функции потерь включают:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4QrNzhaPMFp"
   },
   "source": [
    "## Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZXPI-gvPCbO"
   },
   "source": [
    "Mean Squared Error (MSE): This is used for estimating the value function. It measures the average of the squares of the errors — that is, the average squared difference between the estimated values and what is estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cejOv2xPEjg"
   },
   "source": [
    "Среднеквадратичная ошибка (MSE, Mean Squared Error): Применяется для оценки функции стоимости. Измеряет среднее значение квадратов ошибок — то есть среднеквадратичное отклонение между оценочными значениями и тем, что оценивается. Это позволяет оценить, насколько хорошо модель предсказывает ожидаемую награду."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AurXWNwWPVif"
   },
   "source": [
    "## Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEUjXBSbPYZF"
   },
   "source": [
    "**Cross-Entropy Loss**: This is used for multi-class classification problems. It measures the difference between two probability distributions.\n",
    "\n",
    "**Потеря кросс-энтропии (Cross-Entropy Loss)**: Применяется для задач многоклассовой классификации. Измеряет разницу между двуми распределениями вероятностей, что позволяет оценить, насколько хорошо модель предсказывает вероятности каждого класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPQDJ4EGP02l"
   },
   "source": [
    "# **4. Генеративные модели (Generative Models)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLnwxCbWPu50"
   },
   "source": [
    "\n",
    "В этой области происходит генерация новых экземпляров данных. Некоторые часто используемые функции потерь включают:\n",
    "\n",
    "This domain involves generating new data instances. Some commonly used loss functions include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKKcsVPkQIdz"
   },
   "source": [
    "## Kullback-Leibler (KL) Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbAsLeUEQDL-"
   },
   "source": [
    "Kullback-Leibler (KL) Divergence: This is used in Variational Autoencoders (VAEs). It measures how one probability distribution differs from another.\n",
    "\n",
    "Дивергенция Кульбака-Лейблера (KL, Kullback-Leibler Divergence): Применяется в вариационных автоэнкодерах (VAE). Измеряет, насколько одно распределение вероятностей отличается от другого, что позволяет оценить, насколько хорошо модель предсказывает распределение данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1705408227615,
     "user": {
      "displayName": "Maxim Sorokin",
      "userId": "04915505168425669858"
     },
     "user_tz": -180
    },
    "id": "wFL1VYwWQCxM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def kl_divergence_manual(p, q):\n",
    "    \"\"\"\n",
    "    Manually implemented Kullback-Leibler (KL) Divergence function.\n",
    "\n",
    "    Parameters:\n",
    "    p (numpy.ndarray): A numpy array representing the true probability distribution.\n",
    "    q (numpy.ndarray): A numpy array representing the approximated probability distribution.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated KL divergence.\n",
    "    \"\"\"\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "def kl_divergence_torch(p, q):\n",
    "    \"\"\"\n",
    "    Kullback-Leibler (KL) Divergence function using PyTorch's built-in function.\n",
    "\n",
    "    Parameters:\n",
    "    p (torch.Tensor): A tensor representing the true probability distribution.\n",
    "    q (torch.Tensor): A tensor representing the approximated probability distribution.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A tensor containing the KL divergence.\n",
    "    \"\"\"\n",
    "    kl_div = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "    return kl_div(torch.log(q), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkNPieVTQYFV"
   },
   "source": [
    "## Wasserstein Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khFVctSMQXKs"
   },
   "source": [
    "Wasserstein Loss: This is used in Wasserstein GANs. It measures the distance between the data distribution observed in the training dataset and the distribution observed in the generated examples.\n",
    "\n",
    "Потеря Вассерштейна (Wasserstein Loss): Применяется в Вассерштейновских GAN. Измеряет расстояние между распределением данных, наблюдаемым в обучающем наборе данных, и распределением, наблюдаемым в сгенерированных примерах, что позволяет оценить, насколько хорошо модель воспроизводит распределение данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEo3mlGnQWiH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def wasserstein_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Wasserstein Loss function used in Wasserstein GANs.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred (torch.Tensor): A tensor representing the predicted values.\n",
    "    y_true (torch.Tensor): A tensor containing the true values.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A tensor containing the Wasserstein loss.\n",
    "    \"\"\"\n",
    "    return torch.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sx4xUkcrQ0ss"
   },
   "source": [
    "## Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7orPDMccQwjF"
   },
   "source": [
    "Binary Cross-Entropy Loss: This is used in Generative Adversarial Networks (GANs). It measures the dissimilarity between the predicted probabilities and the true binary labels.\n",
    "\n",
    "Бинарная потеря кросс-энтропии (Binary Cross-Entropy Loss): Применяется в генеративно-состязательных сетях (GAN). Измеряет несоответствие между предсказанными вероятностями и истинными бинарными метками, что позволяет оценить, насколько хорошо модель предсказывает истинность сгенерированных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO59hyqMQ7i0"
   },
   "source": [
    "# **5. Области решения задач (Problem-Solving Domains)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gQAo2chQ6rr"
   },
   "source": [
    "These domains focus on using machine learning to solve specific problems. The choice of loss function depends on the specific task. For example:\n",
    "\n",
    "Эти области сосредоточены на использовании машинного обучения для решения конкретных задач. Выбор функции потерь зависит от конкретной задачи. Например:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbzvBahVRTok"
   },
   "source": [
    "## Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1koi1LnfRSlA"
   },
   "source": [
    "Mean Squared Error (MSE): This is used for regression tasks. It measures the average of the squares of the errors — that is, the average squared difference between the estimated values and what is estimated.\n",
    "\n",
    "\n",
    "Среднеквадратичная ошибка (MSE, Mean Squared Error): Применяется для задач регрессии. Измеряет среднее значение квадратов ошибок — то есть среднеквадратичное отклонение между оценочными значениями и тем, что оценивается. Это позволяет оценить, насколько хорошо модель предсказывает непрерывные значения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHThB267RVL-"
   },
   "source": [
    "## Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-Skysx3RIwu"
   },
   "source": [
    "Cross-Entropy Loss: This is used for classification tasks. It measures the difference between two probability distributions.\n",
    "\n",
    "\n",
    "Потеря кросс-энтропии (Cross-Entropy Loss): Применяется для задач классификации. Измеряет разницу между двуми распределениями вероятностей, что позволяет оценить, насколько хорошо модель предсказывает вероятности каждого класса.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jbQnRhERWsq"
   },
   "source": [
    "## Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFb-6CoiRI49"
   },
   "source": [
    "Hinge Loss: This is used for “maximum-margin” classification. It measures the difference between the predicted and the actual output.\n",
    "\n",
    "Потеря на петлях (Hinge Loss): Применяется для классификации с “максимальным зазором”. Измеряет разницу между предсказанным и фактическим выходом, что позволяет оценить, насколько хорошо модель разделяет классы."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPuJFcmFelSnk9vM7iDCUBO",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

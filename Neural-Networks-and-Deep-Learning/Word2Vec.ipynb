{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMeFZehykjxUN0ui/usNvaf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Word2Vec\n","\n","Word2Vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words.\n","\n","## How it Works\n","\n","Word2Vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space.\n","\n","Word2Vec is actually a collection of two different methods: Continuous Bag-of-Words (CBOW) and Skip-Gram. In the CBOW method, the model predicts a word given the surrounding context. In the Skip-Gram method, the model predicts the context given a word.\n","\n","## Use Cases\n","\n","Word2Vec has a wide range of applications, including:\n","\n","- **Natural Language Processing (NLP)**: Word2Vec is used for various NLP tasks, such as sentiment analysis, named entity recognition, and machine translation.\n","\n","- **Information Retrieval**: Word2Vec can improve the performance of information retrieval systems by considering the semantic similarity of words.\n","\n","- **Recommendation Systems**: Word2Vec can be used to improve recommendation systems by considering the semantic similarity of items.\n","\n","## Limitations\n","\n","Despite their advantages, Word2Vec also has some limitations:\n","\n","- **Lack of Semantic Understanding**: Word2Vec maps semantically similar words close to each other, but it doesn't understand the meaning of the words.\n","\n","- **Contextual Ambiguity**: Word2Vec gives the same vector representation for a word regardless of its context, which can be a problem for words with multiple meanings.\n","\n","- **Interpretability**: Like other deep learning models, Word2Vec can be seen as a \"black box\" and can be difficult to interpret. This can make it challenging to understand why a particular prediction was made."],"metadata":{"id":"DFSiM8-1BZy3"}},{"cell_type":"markdown","source":["# Word2Vec\n","\n","Word2Vec - это группа связанных моделей, используемых для создания векторных представлений слов. Эти модели представляют собой поверхностные двухслойные нейронные сети, которые обучаются восстанавливать лингвистические контексты слов.\n","\n","## Как это работает\n","\n","Word2Vec принимает на вход большой корпус текста и создает векторное пространство, обычно нескольких сотен измерений, где каждому уникальному слову в корпусе присваивается соответствующий вектор в пространстве. Векторы слов располагаются в векторном пространстве таким образом, что слова, которые имеют общие контексты в корпусе, расположены близко друг к другу в пространстве.\n","\n","Word2Vec на самом деле представляет собой коллекцию двух разных методов: Continuous Bag-of-Words (CBOW) и Skip-Gram. В методе CBOW модель предсказывает слово, учитывая окружающий контекст. В методе Skip-Gram модель предсказывает контекст, учитывая слово.\n","\n","## Варианты использования\n","\n","У Word2Vec есть широкий спектр применений, включая:\n","\n","- **Обработка естественного языка (NLP)**: Word2Vec используется для различных задач NLP, таких как моделирование языка, машинный перевод и анализ тональности.\n","\n","- **Извлечение информации**: Word2Vec может улучшить производительность систем извлечения информации, учитывая семантическую схожесть слов.\n","\n","- **Системы рекомендаций**: Word2Vec может использоваться для улучшения систем рекомендаций, учитывая семантическую схожесть элементов.\n","\n","## Ограничения\n","\n","Несмотря на их преимущества, у Word2Vec также есть некоторые ограничения:\n","\n","- **Отсутствие семантического понимания**: Word2Vec отображает семантически похожие слова близко друг к другу, но он не понимает значение слов.\n","\n","- **Контекстуальная неоднозначность**: Word2Vec дает одинаковое векторное представление для слова, независимо от его контекста, что может быть проблемой для слов с несколькими значениями.\n","\n","- **Интерпретируемость**: Как и другие модели глубокого обучения, Word2Vec может рассматриваться как \"черный ящик\" и может быть сложным для интерпретации. Это может затруднить понимание причин, по которым был сделан определенный прогноз."],"metadata":{"id":"li2EuAjiBZ0_"}},{"cell_type":"code","source":["try:\n","    import gensim\n","    from nltk.corpus import brown\n","except ImportError:\n","    import os\n","    os.system('pip install gensim nltk')\n","    # Import the gensim library, which provides functionality for training Word2Vec models\n","    import gensim\n","    # Import the Brown corpus from the NLTK library. The Brown corpus is a collection of text samples from a wide range of sources, with a total of over a million words.\n","    from nltk.corpus import brown\n","\n","\n","class Word2VecModel:\n","    \"\"\"\n","    This class implements a Word2Vec model using the Gensim library.\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"\n","        Constructor for the Word2VecModel class.\n","        Initializes the model to None.\n","        \"\"\"\n","        self.model = None\n","\n","    def train(self, sentences):\n","        \"\"\"\n","        Trains the Word2Vec model on the given sentences.\n","\n","        Args:\n","            sentences (list of list of str): The sentences to train on. Each sentence is a list of words.\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        # Train the model on the sentences\n","        self.model = gensim.models.Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n","\n","    def get_vector(self, word):\n","        \"\"\"\n","        Gets the vector for a word.\n","\n","        Args:\n","            word (str): The word to get the vector for.\n","\n","        Returns:\n","            numpy.ndarray: The vector for the word.\n","        \"\"\"\n","        # Check if the model has been trained\n","        if self.model is None:\n","            raise ValueError(\"The model has not been trained yet.\")\n","\n","        # Return the vector for the word\n","        return self.model.wv[word]\n","\n","# Download the Brown corpus if it's not already downloaded\n","try:\n","    brown.sents()\n","except LookupError:\n","    import nltk\n","    nltk.download('brown')\n","\n","# Create an instance of the Word2VecModel class\n","model = Word2VecModel()\n","\n","# Train the model on the sentences from the Brown corpus\n","model.train(brown.sents())\n","\n","# Get the vector for a word\n","vector = model.get_vector('word')"],"metadata":{"id":"2NHJE3r3BZ7f","executionInfo":{"status":"ok","timestamp":1702734258660,"user_tz":-180,"elapsed":26763,"user":{"displayName":"Maxim Sorokin","userId":"04915505168425669858"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["For evaluating a trained Word2Vec model, one common approach is to check the quality of the learned word vectors. This can be done by checking how similar the vectors of semantically similar words are."],"metadata":{"id":"VWJpEp5R4THm"}},{"cell_type":"markdown","source":["Для оценки обученной модели Word2Vec одним из распространенных подходов является проверка качества изученных векторов слов. Это можно сделать, проверив, насколько похожи векторы семантически похожих слов."],"metadata":{"id":"vn7eOF8N4UVG"}},{"cell_type":"code","source":["# Get the most similar words to 'word'\n","similar_words = model.model.wv.most_similar('word')\n","\n","# Print the similar words\n","for word, similarity in similar_words:\n","    print(f\"Word: {word}, Similarity: {similarity}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dz0oJQ7y2hfC","executionInfo":{"status":"ok","timestamp":1702734426606,"user_tz":-180,"elapsed":4,"user":{"displayName":"Maxim Sorokin","userId":"04915505168425669858"}},"outputId":"624d5caf-53e4-4aea-843e-25e018439be3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Word: child, Similarity: 0.9698866605758667\n","Word: person, Similarity: 0.9421505331993103\n","Word: story, Similarity: 0.9303193092346191\n","Word: truth, Similarity: 0.9184790253639221\n","Word: question, Similarity: 0.9174147248268127\n","Word: idea, Similarity: 0.9156782627105713\n","Word: picture, Similarity: 0.9155687689781189\n","Word: job, Similarity: 0.9154903292655945\n","Word: situation, Similarity: 0.9144237637519836\n","Word: statement, Similarity: 0.9139750599861145\n"]}]}]}